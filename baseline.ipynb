{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human_readable (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "using LinearAlgebra\n",
    "using NBInclude\n",
    "@nbinclude(\"deal_features.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "minimize(func, data, optimizer=Adam(); params)\n",
       "sgd     (func, data; lr=0.1,  gclip, params)\n",
       "momentum(func, data; lr=0.05, gamma=0.95, gclip, params)\n",
       "nesterov(func, data; lr=0.05, gamma=0.95, gclip, params)\n",
       "adagrad (func, data; lr=0.05, eps=1e-6, gclip, params)\n",
       "rmsprop (func, data; lr=0.01, rho=0.9, eps=1e-6, gclip, params)\n",
       "adadelta(func, data; lr=1.0,  rho=0.9, eps=1e-6, gclip, params)\n",
       "adam    (func, data; lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gclip, params)\n",
       "\\end{verbatim}\n",
       "Return an iterator which applies \\texttt{func} to arguments in \\texttt{data}, i.e.  \\texttt{(func(args...) for args in data)}, and updates the parameters every iteration to minimize \\texttt{func}.  \\texttt{func} should return a scalar value.\n",
       "\n",
       "The common keyword argument \\texttt{params} can be used to list the \\texttt{Param}s to be optimized.  If not specified, any \\texttt{Param} that takes part in the computation of \\texttt{func(args...)} will be updated.\n",
       "\n",
       "The common keyword argument \\texttt{gclip} can be used to implement per-parameter gradient clipping. For a parameter gradient \\texttt{g}, if \\texttt{norm(g) > gclip > 0}, \\texttt{g} is scaled so that its norm is equal to \\texttt{gclip}. If not specified no gradient clipping is performed.\n",
       "\n",
       "These functions do not perform optimization, but return an iterator that can. Any function that produces values from an iterator can be used with such an object, e.g. \\texttt{progress!(sgd(f,d))} iterates the sgd optimizer and displays a progress bar. For convenience, appending \\texttt{!} to the name of the function iterates and returns \\texttt{nothing}, i.e. \\texttt{sgd!(...)} is equivalent to \\texttt{(for x in sgd(...) end)}.\n",
       "\n",
       "We define optimizers as lazy iterators to have explicit control over them:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item To report progress use \\texttt{progress(sgd(f,d))}.\n",
       "\n",
       "\n",
       "\\item To run until convergence use \\texttt{converge(sgd(f,cycle(d)))}.\n",
       "\n",
       "\n",
       "\\item To run multiple epochs use \\texttt{sgd(f,repeat(d,n))}.\n",
       "\n",
       "\n",
       "\\item To run a given number of iterations use \\texttt{sgd(f,take(cycle(d),n))}.\n",
       "\n",
       "\n",
       "\\item To do a task every n iterations use \\texttt{(task() for (i,j) in enumerate(sgd(f,d)) if i\\%n == 1)}.\n",
       "\n",
       "\\end{itemize}\n",
       "These functions apply the same algorithm with the same configuration to every parameter by default. \\texttt{minimize} takes an explicit optimizer argument, all others call \\texttt{minimize} with an appropriate optimizer argument (see \\texttt{@doc update!} for a list of possible optimizers). Before calling \\href{@ref}{\\texttt{update!}} on a \\texttt{Param}, \\texttt{minimize} sets its \\texttt{opt} field to a copy of this default optimizer if it is not already set. The \\texttt{opt} field is used by the \\texttt{update!} function to determine the type of update performed on that parameter.  If you need finer grained control, you can set the optimizer of an individual \\texttt{Param} by setting its \\texttt{opt} field before calling one of these functions. They will not override the \\texttt{opt} field if it is already set, e.g. \\texttt{sgd(model,data)} will perform an \\texttt{Adam} update for a parameter whose \\texttt{opt} field is an \\texttt{Adam} object. This also means you can stop and start the training without losing optimization state, the first call will set the \\texttt{opt} fields and the subsequent calls will not override them.\n",
       "\n",
       "Given a parameter \\texttt{w} and its gradient \\texttt{g} here are the updates applied by each optimizer:\n",
       "\n",
       "\\begin{verbatim}\n",
       "# sgd (http://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
       "w .= w - lr * g\n",
       "\n",
       "# momentum (http://jlmelville.github.io/mize/nesterov.html)\n",
       "v .= gamma * v - lr * g\n",
       "w .= w + v\n",
       "\n",
       "# nesterov (http://jlmelville.github.io/mize/nesterov.html)\n",
       "w .= w - gamma * v\n",
       "v .= gamma * v - lr * g\n",
       "w .= w + (1 + gamma) * v\n",
       "\n",
       "# adagrad (http://www.jmlr.org/papers/v12/duchi11a.html)\n",
       "G .= G + g .^ 2\n",
       "w .= w - lr * g ./ sqrt(G + eps)\n",
       "\n",
       "# rmsprop (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
       "G .= rho * G + (1-rho) * g .^ 2 \n",
       "w .= w - lr * g ./ sqrt(G + eps)\n",
       "\n",
       "# adadelta (http://arxiv.org/abs/1212.5701)\n",
       "G .= rho * G + (1-rho) * g .^ 2\n",
       "update = sqrt(delta + eps) .* g ./ sqrt(G + eps)\n",
       "w = w - lr * update\n",
       "delta = rho * delta + (1-rho) * update .^ 2\n",
       "\n",
       "# adam (http://arxiv.org/abs/1412.6980)\n",
       "v = beta1 * v + (1 - beta1) * g\n",
       "G = beta2 * G + (1 - beta2) * g .^ 2\n",
       "vhat = v ./ (1 - beta1 ^ t)\n",
       "Ghat = G ./ (1 - beta2 ^ t)\n",
       "w = w - (lr / (sqrt(Ghat) + eps)) * vhat\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "minimize(func, data, optimizer=Adam(); params)\n",
       "sgd     (func, data; lr=0.1,  gclip, params)\n",
       "momentum(func, data; lr=0.05, gamma=0.95, gclip, params)\n",
       "nesterov(func, data; lr=0.05, gamma=0.95, gclip, params)\n",
       "adagrad (func, data; lr=0.05, eps=1e-6, gclip, params)\n",
       "rmsprop (func, data; lr=0.01, rho=0.9, eps=1e-6, gclip, params)\n",
       "adadelta(func, data; lr=1.0,  rho=0.9, eps=1e-6, gclip, params)\n",
       "adam    (func, data; lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gclip, params)\n",
       "```\n",
       "\n",
       "Return an iterator which applies `func` to arguments in `data`, i.e.  `(func(args...) for args in data)`, and updates the parameters every iteration to minimize `func`.  `func` should return a scalar value.\n",
       "\n",
       "The common keyword argument `params` can be used to list the `Param`s to be optimized.  If not specified, any `Param` that takes part in the computation of `func(args...)` will be updated.\n",
       "\n",
       "The common keyword argument `gclip` can be used to implement per-parameter gradient clipping. For a parameter gradient `g`, if `norm(g) > gclip > 0`, `g` is scaled so that its norm is equal to `gclip`. If not specified no gradient clipping is performed.\n",
       "\n",
       "These functions do not perform optimization, but return an iterator that can. Any function that produces values from an iterator can be used with such an object, e.g. `progress!(sgd(f,d))` iterates the sgd optimizer and displays a progress bar. For convenience, appending `!` to the name of the function iterates and returns `nothing`, i.e. `sgd!(...)` is equivalent to `(for x in sgd(...) end)`.\n",
       "\n",
       "We define optimizers as lazy iterators to have explicit control over them:\n",
       "\n",
       "  * To report progress use `progress(sgd(f,d))`.\n",
       "  * To run until convergence use `converge(sgd(f,cycle(d)))`.\n",
       "  * To run multiple epochs use `sgd(f,repeat(d,n))`.\n",
       "  * To run a given number of iterations use `sgd(f,take(cycle(d),n))`.\n",
       "  * To do a task every n iterations use `(task() for (i,j) in enumerate(sgd(f,d)) if i%n == 1)`.\n",
       "\n",
       "These functions apply the same algorithm with the same configuration to every parameter by default. `minimize` takes an explicit optimizer argument, all others call `minimize` with an appropriate optimizer argument (see `@doc update!` for a list of possible optimizers). Before calling [`update!`](@ref) on a `Param`, `minimize` sets its `opt` field to a copy of this default optimizer if it is not already set. The `opt` field is used by the `update!` function to determine the type of update performed on that parameter.  If you need finer grained control, you can set the optimizer of an individual `Param` by setting its `opt` field before calling one of these functions. They will not override the `opt` field if it is already set, e.g. `sgd(model,data)` will perform an `Adam` update for a parameter whose `opt` field is an `Adam` object. This also means you can stop and start the training without losing optimization state, the first call will set the `opt` fields and the subsequent calls will not override them.\n",
       "\n",
       "Given a parameter `w` and its gradient `g` here are the updates applied by each optimizer:\n",
       "\n",
       "```\n",
       "# sgd (http://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
       "w .= w - lr * g\n",
       "\n",
       "# momentum (http://jlmelville.github.io/mize/nesterov.html)\n",
       "v .= gamma * v - lr * g\n",
       "w .= w + v\n",
       "\n",
       "# nesterov (http://jlmelville.github.io/mize/nesterov.html)\n",
       "w .= w - gamma * v\n",
       "v .= gamma * v - lr * g\n",
       "w .= w + (1 + gamma) * v\n",
       "\n",
       "# adagrad (http://www.jmlr.org/papers/v12/duchi11a.html)\n",
       "G .= G + g .^ 2\n",
       "w .= w - lr * g ./ sqrt(G + eps)\n",
       "\n",
       "# rmsprop (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
       "G .= rho * G + (1-rho) * g .^ 2 \n",
       "w .= w - lr * g ./ sqrt(G + eps)\n",
       "\n",
       "# adadelta (http://arxiv.org/abs/1212.5701)\n",
       "G .= rho * G + (1-rho) * g .^ 2\n",
       "update = sqrt(delta + eps) .* g ./ sqrt(G + eps)\n",
       "w = w - lr * update\n",
       "delta = rho * delta + (1-rho) * update .^ 2\n",
       "\n",
       "# adam (http://arxiv.org/abs/1412.6980)\n",
       "v = beta1 * v + (1 - beta1) * g\n",
       "G = beta2 * G + (1 - beta2) * g .^ 2\n",
       "vhat = v ./ (1 - beta1 ^ t)\n",
       "Ghat = G ./ (1 - beta2 ^ t)\n",
       "w = w - (lr / (sqrt(Ghat) + eps)) * vhat\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  minimize(func, data, optimizer=Adam(); params)\u001b[39m\n",
       "\u001b[36m  sgd     (func, data; lr=0.1,  gclip, params)\u001b[39m\n",
       "\u001b[36m  momentum(func, data; lr=0.05, gamma=0.95, gclip, params)\u001b[39m\n",
       "\u001b[36m  nesterov(func, data; lr=0.05, gamma=0.95, gclip, params)\u001b[39m\n",
       "\u001b[36m  adagrad (func, data; lr=0.05, eps=1e-6, gclip, params)\u001b[39m\n",
       "\u001b[36m  rmsprop (func, data; lr=0.01, rho=0.9, eps=1e-6, gclip, params)\u001b[39m\n",
       "\u001b[36m  adadelta(func, data; lr=1.0,  rho=0.9, eps=1e-6, gclip, params)\u001b[39m\n",
       "\u001b[36m  adam    (func, data; lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gclip, params)\u001b[39m\n",
       "\n",
       "  Return an iterator which applies \u001b[36mfunc\u001b[39m to arguments in \u001b[36mdata\u001b[39m, i.e.\n",
       "  \u001b[36m(func(args...) for args in data)\u001b[39m, and updates the parameters every iteration\n",
       "  to minimize \u001b[36mfunc\u001b[39m. \u001b[36mfunc\u001b[39m should return a scalar value.\n",
       "\n",
       "  The common keyword argument \u001b[36mparams\u001b[39m can be used to list the \u001b[36mParam\u001b[39ms to be\n",
       "  optimized. If not specified, any \u001b[36mParam\u001b[39m that takes part in the computation of\n",
       "  \u001b[36mfunc(args...)\u001b[39m will be updated.\n",
       "\n",
       "  The common keyword argument \u001b[36mgclip\u001b[39m can be used to implement per-parameter\n",
       "  gradient clipping. For a parameter gradient \u001b[36mg\u001b[39m, if \u001b[36mnorm(g) > gclip > 0\u001b[39m, \u001b[36mg\u001b[39m is\n",
       "  scaled so that its norm is equal to \u001b[36mgclip\u001b[39m. If not specified no gradient\n",
       "  clipping is performed.\n",
       "\n",
       "  These functions do not perform optimization, but return an iterator that\n",
       "  can. Any function that produces values from an iterator can be used with\n",
       "  such an object, e.g. \u001b[36mprogress!(sgd(f,d))\u001b[39m iterates the sgd optimizer and\n",
       "  displays a progress bar. For convenience, appending \u001b[36m!\u001b[39m to the name of the\n",
       "  function iterates and returns \u001b[36mnothing\u001b[39m, i.e. \u001b[36msgd!(...)\u001b[39m is equivalent to \u001b[36m(for\n",
       "  x in sgd(...) end)\u001b[39m.\n",
       "\n",
       "  We define optimizers as lazy iterators to have explicit control over them:\n",
       "\n",
       "    •    To report progress use \u001b[36mprogress(sgd(f,d))\u001b[39m.\n",
       "\n",
       "    •    To run until convergence use \u001b[36mconverge(sgd(f,cycle(d)))\u001b[39m.\n",
       "\n",
       "    •    To run multiple epochs use \u001b[36msgd(f,repeat(d,n))\u001b[39m.\n",
       "\n",
       "    •    To run a given number of iterations use \u001b[36msgd(f,take(cycle(d),n))\u001b[39m.\n",
       "\n",
       "    •    To do a task every n iterations use \u001b[36m(task() for (i,j) in\n",
       "        enumerate(sgd(f,d)) if i%n == 1)\u001b[39m.\n",
       "\n",
       "  These functions apply the same algorithm with the same configuration to\n",
       "  every parameter by default. \u001b[36mminimize\u001b[39m takes an explicit optimizer argument,\n",
       "  all others call \u001b[36mminimize\u001b[39m with an appropriate optimizer argument (see \u001b[36m@doc\n",
       "  update!\u001b[39m for a list of possible optimizers). Before calling \u001b[36mupdate!\u001b[39m on a\n",
       "  \u001b[36mParam\u001b[39m, \u001b[36mminimize\u001b[39m sets its \u001b[36mopt\u001b[39m field to a copy of this default optimizer if it\n",
       "  is not already set. The \u001b[36mopt\u001b[39m field is used by the \u001b[36mupdate!\u001b[39m function to\n",
       "  determine the type of update performed on that parameter. If you need finer\n",
       "  grained control, you can set the optimizer of an individual \u001b[36mParam\u001b[39m by setting\n",
       "  its \u001b[36mopt\u001b[39m field before calling one of these functions. They will not override\n",
       "  the \u001b[36mopt\u001b[39m field if it is already set, e.g. \u001b[36msgd(model,data)\u001b[39m will perform an\n",
       "  \u001b[36mAdam\u001b[39m update for a parameter whose \u001b[36mopt\u001b[39m field is an \u001b[36mAdam\u001b[39m object. This also\n",
       "  means you can stop and start the training without losing optimization state,\n",
       "  the first call will set the \u001b[36mopt\u001b[39m fields and the subsequent calls will not\n",
       "  override them.\n",
       "\n",
       "  Given a parameter \u001b[36mw\u001b[39m and its gradient \u001b[36mg\u001b[39m here are the updates applied by each\n",
       "  optimizer:\n",
       "\n",
       "\u001b[36m  # sgd (http://en.wikipedia.org/wiki/Stochastic_gradient_descent)\u001b[39m\n",
       "\u001b[36m  w .= w - lr * g\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # momentum (http://jlmelville.github.io/mize/nesterov.html)\u001b[39m\n",
       "\u001b[36m  v .= gamma * v - lr * g\u001b[39m\n",
       "\u001b[36m  w .= w + v\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # nesterov (http://jlmelville.github.io/mize/nesterov.html)\u001b[39m\n",
       "\u001b[36m  w .= w - gamma * v\u001b[39m\n",
       "\u001b[36m  v .= gamma * v - lr * g\u001b[39m\n",
       "\u001b[36m  w .= w + (1 + gamma) * v\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # adagrad (http://www.jmlr.org/papers/v12/duchi11a.html)\u001b[39m\n",
       "\u001b[36m  G .= G + g .^ 2\u001b[39m\n",
       "\u001b[36m  w .= w - lr * g ./ sqrt(G + eps)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # rmsprop (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\u001b[39m\n",
       "\u001b[36m  G .= rho * G + (1-rho) * g .^ 2 \u001b[39m\n",
       "\u001b[36m  w .= w - lr * g ./ sqrt(G + eps)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # adadelta (http://arxiv.org/abs/1212.5701)\u001b[39m\n",
       "\u001b[36m  G .= rho * G + (1-rho) * g .^ 2\u001b[39m\n",
       "\u001b[36m  update = sqrt(delta + eps) .* g ./ sqrt(G + eps)\u001b[39m\n",
       "\u001b[36m  w = w - lr * update\u001b[39m\n",
       "\u001b[36m  delta = rho * delta + (1-rho) * update .^ 2\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # adam (http://arxiv.org/abs/1412.6980)\u001b[39m\n",
       "\u001b[36m  v = beta1 * v + (1 - beta1) * g\u001b[39m\n",
       "\u001b[36m  G = beta2 * G + (1 - beta2) * g .^ 2\u001b[39m\n",
       "\u001b[36m  vhat = v ./ (1 - beta1 ^ t)\u001b[39m\n",
       "\u001b[36m  Ghat = G ./ (1 - beta2 ^ t)\u001b[39m\n",
       "\u001b[36m  w = w - (lr / (sqrt(Ghat) + eps)) * vhat\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.rmsprop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_random_instances (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_cost(hands_NS,random_trial=5;random_cost=false)\n",
    "    if random_cost\n",
    "        return rand(52,1)\n",
    "    end\n",
    "    costs=zeros(52,1)\n",
    "    for i in 1:random_trial\n",
    "        deal=random_deal(hands)\n",
    "        #---- generate cost vector here\n",
    "        \n",
    "        #-----\n",
    "        costs +=costs\n",
    "    end\n",
    "    return costs./random_trial\n",
    "end\n",
    "function generate_random_instances(size)\n",
    "    data=Array{Float64,3}(undef,3,52,size)\n",
    "    for i in 1:size\n",
    "        deal=random_deal()\n",
    "        data[1,:,i]=deal[:,1]\n",
    "        data[2,:,i]=deal[:,2]\n",
    "        data[3,:,i]=generate_cost(deal[:,1:2],random_cost=true)\n",
    "    end\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.481380115073463"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_size=1000;\n",
    "instances = generate_random_instances(instance_size);\n",
    "turn_depth=6;\n",
    "costs=[]\n",
    "iteration_size=1000\n",
    "for i in 1:iteration_size\n",
    "    instance= instances[:,:,rand(1:instance_size)]; #choose a random deal\n",
    "    bidding_hist=zeros(36);\n",
    "    for t in 1:turn_depth\n",
    "        if t==1\n",
    "            last_bid=1\n",
    "        else\n",
    "            last_bid=maximum(findall(x-> x==1,bidding_hist)) #the nature of an action\n",
    "        end\n",
    "        current_bid=rand(last_bid:36) #randomly bid a contract\n",
    "        if last_bid == current_bid #if current bid is pass\n",
    "            break;\n",
    "        end\n",
    "        bidding_hist[current_bid]=1;\n",
    "    end\n",
    "    \n",
    "    contract=1\n",
    "    try\n",
    "        contract=findall(x-> x==1,bidding_hist)[1]\n",
    "    catch\n",
    "        contract=1\n",
    "    end\n",
    "    cost=instance[3,contract]   # cost of declaring this contract\n",
    "    push!(costs,cost)\n",
    "end\n",
    "\n",
    "mean(costs)   # overall success of the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
